{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入我们所需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./'\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "# os.system('pip install numba')\n",
    "from numba import jit\n",
    "#tqdm\n",
    "# os.system('pip install tqdm')\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Integrated model\n",
    "# os.system('pip install lightgbm')\n",
    "import lightgbm as lgb\n",
    "# os.system('pip install catboost==0.15.2')\n",
    "import catboost as cbt\n",
    "# os.system('pip install xgboost')\n",
    "# import xgboost as xgb\n",
    "\n",
    "#base import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# about sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler as std\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import f1_score\n",
    "#about time\n",
    "import time\n",
    "import datetime \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Garbage collection\n",
    "import gc\n",
    "# scipy\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "import scipy.spatial.distance as dist\n",
    "#other\n",
    "from collections import Counter \n",
    "from statistics import mode \n",
    "#warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json \n",
    "import math\n",
    "from itertools import product\n",
    "import ast \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我在构建训练集的时候，我把复赛训练集也拿来训练了（丰富了训练集）  \n",
    "round1_iflyad_anticheat_traindata.txt为初赛的训练集   \n",
    "round2_iflyad_anticheat_traindata.txt为复赛的训练集     \n",
    "使用pd.read_table读取txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_concat\n",
    "train1 = pd.read_table(path+\"round1_iflyad_anticheat_traindata.txt\")\n",
    "train2 = pd.read_table(path+\"round2_iflyad_anticheat_traindata.txt\")\n",
    "train = train1.append(train2).reset_index(drop=True)#reset_index重建索引，drop=True 是把原来索引删掉\n",
    "###concat  会报错，害我浪费了一些时间\n",
    "#train = pd.concat([train1,train2])\n",
    "#打印训练集训练\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "round2_iflyad_anticheat_testdata_feature_A.txt 是复赛需要预测的A榜数据集   \n",
    "round2_iflyad_anticheat_testdata_feature_B.txt 是复赛需要预测的B榜数据集   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2a = pd.read_table(path+\"round2_iflyad_anticheat_testdata_feature_A.txt\")\n",
    "test_2b = pd.read_table(path+\"round2_iflyad_anticheat_testdata_feature_B.txt\")\n",
    "test = test_2a.append(test_2b).reset_index(drop=True)#reset_index重建索引，drop=True 是把原来索引删掉\n",
    "#test = pd.concat([test_2a,test_2b])\n",
    "print(test.info())\n",
    "test['label'] = -999 #这样的方式是填充test 的label 列，使得test和train对其\n",
    "data = train.append(test).reset_index(drop=True)#reset_index重建索引，drop=True 是把原来索引删掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train,test])\n",
    "# 数据预处理\n",
    "## 制造商预处理\n",
    "#将手机厂商 make列包含某一列的的make改成统一的类型\n",
    "data.loc[data['make'].str.contains('长虹', na=False), 'make'] = 'changhong'\n",
    "data.loc[data['make'].str.contains('朵唯', na=False), 'make'] = 'doov'\n",
    "data.loc[data['make'].str.contains('sm', na=False), 'make'] = 'samsung'\n",
    "data.loc[data['make'].str.contains('SAMSUNG', na=False), 'make'] = 'samsung'\n",
    "data.loc[data['make'].str.contains('三星', na=False), 'make'] = 'samsung'\n",
    "data.loc[data['make'].str.contains('GT-', na=False), 'make'] = 'samsung'\n",
    "data.loc[data['make'].str.contains('格力', na=False), 'make'] = 'gree'\n",
    "data.loc[data['make'].str.contains('Moto G', na=False), 'make'] = 'motorola'\n",
    "data.loc[data['make'].str.contains('Moto', na=False), 'make'] = 'motorola'\n",
    "data.loc[data['make'].str.contains('moto', na=False), 'make'] = 'motorola'\n",
    "data.loc[data['make'].str.contains('摩托罗拉', na=False), 'make'] = 'motorola'\n",
    "data.loc[data['make'].str.contains('诺基亚', na=False), 'make'] = 'nokia'\n",
    "data.loc[data['make'].str.contains('Nokia', na=False), 'make'] = 'nokia'\n",
    "data.loc[data['make'].str.contains('努比亚', na=False), 'make'] = 'nubia'\n",
    "data.loc[data['make'].str.contains('美图', na=False), 'make'] = 'meitu'\n",
    "data.loc[data['make'].str.contains('LG-', na=False), 'make'] = 'LG'\n",
    "data.loc[data['make'].str.contains('联想', na=False), 'make'] = 'lenovo'\n",
    "data.loc[data['make'].str.contains('rv:', na=False), 'make'] = 'RV'\n",
    "data.loc[data['make'].str.contains('rv:', na=False), 'make'] = 'RV'\n",
    "data.loc[data['make'].str.contains('小辣椒', na=False), 'make'] = 'xiaolajiao'\n",
    "data.loc[data['make'].str.contains('HUAWEI', na=False), 'make'] = 'huawei'\n",
    "data.loc[data['make'].str.contains('huawei', na=False), 'make'] = 'huawei'\n",
    "data.loc[data['make'].str.contains('荣耀', na=False), 'make'] = 'huawei'\n",
    "data.loc[data['make'].str.contains('华为', na=False), 'make'] = 'huawei'\n",
    "data.loc[data['make'].str.contains('-L', na=False), 'make'] = 'huawei'\n",
    "data.loc[data['make'].str.contains('al', na=False), 'make'] = 'huawei'\n",
    "data.loc[data['make'].str.contains('Blade', na=False), 'make'] = 'zte'\n",
    "data.loc[data['make'].str.contains('BLADE', na=False), 'make'] = 'zte'\n",
    "data.loc[data['make'].str.contains('中兴', na=False), 'make'] = 'zte'\n",
    "data.loc[data['make'].str.contains('海信', na=False), 'make'] = 'hisense'\n",
    "data.loc[data['make'].str.contains('Linux', na=False), 'make'] = 'Linux'\n",
    "data.loc[data['make'].str.contains('乐丰', na=False), 'make'] = 'lephone'\n",
    "data.loc[data['make'].str.contains('百立丰', na=False), 'make'] = 'lephone'\n",
    "data.loc[data['make'].str.contains('乐视', na=False), 'make'] = 'letv'\n",
    "data.loc[data['make'].str.contains('XT', na=False), 'make'] = 'Sony'\n",
    "data.loc[data['make'].str.contains('htc', na=False), 'make'] = 'htc'\n",
    "data.loc[data['make'].str.contains('HTC', na=False), 'make'] = 'htc'\n",
    "data.loc[data['make'].str.contains('ASUS', na=False), 'make'] = 'Asus'\n",
    "data.loc[data['make'].str.contains('锤子', na=False), 'make'] = 'smartisan'\n",
    "data.loc[data['make'].str.contains('oppo', na=False), 'make'] = 'oppo'\n",
    "data.loc[data['make'].str.contains('pb', na=False), 'make'] = 'oppo'\n",
    "data.loc[data['make'].str.contains('realme', na=False), 'make'] = 'oppo'\n",
    "data.loc[data['make'].str.contains('天语', na=False), 'make'] = 'k touch'\n",
    "data.loc[data['make'].str.contains('Tianyu', na=False), 'make'] = 'k touch'\n",
    "data.loc[data['make'].str.contains('tianyu', na=False), 'make'] = 'k touch'\n",
    "data.loc[data['make'].str.contains('酷派', na=False), 'make'] = 'coolpad'\n",
    "data.loc[data['make'].str.contains('索尼', na=False), 'make'] = 'sony'\n",
    "data.loc[data['make'].str.contains('SONY', na=False), 'make'] = 'sony'\n",
    "data.loc[data['make'].str.contains('sony', na=False), 'make'] = 'sony'\n",
    "data.loc[data['make'].str.contains('小米', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('mi', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('m1', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('m1s', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('m2', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('m2s', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('m2a', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('m3', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('m6', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('xiaomi', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('redmi', na=False), 'make'] = 'xiaomi'\n",
    "data.loc[data['make'].str.contains('魅族', na=False), 'make'] = 'meizu'\n",
    "data.loc[data['make'].str.contains('360', na=False), 'make'] = '360'\n",
    "data.loc[data['make'].str.contains('三星', na=False), 'make'] = 'samsung'\n",
    "data.loc[data['make'].str.contains('赛博宇华', na=False), 'make'] = 'sop'\n",
    "data.loc[data['make'].str.contains('金立', na=False), 'make'] = 'jinli'\n",
    "data.loc[data['make'].str.contains('gionee', na=False), 'make'] = 'jinli'\n",
    "data.loc[data['make'].str.contains('Gionee', na=False), 'make'] = 'jinli'\n",
    "data.loc[data['make'].str.contains('vivo', na=False), 'make'] = 'vivo'\n",
    "data.loc[data['make'].str.contains('VIVO', na=False), 'make'] = 'vivo'\n",
    "data.loc[data['make'].str.contains('oneplus', na=False), 'make'] = 'oneplus'\n",
    "data.loc[data['make'].str.contains('一加', na=False), 'make'] = 'oneplus'\n",
    "## 开源预处理 型号预处理\n",
    "#将一些无处理的手机型号替代成能识别的手机信号\n",
    "data['model'].replace('PACM00',\"OPPO R15\",inplace=True)\n",
    "data['model'].replace('PBAM00',\"OPPO A5\",inplace=True)\n",
    "data['model'].replace('PBEM00',\"OPPO R17\",inplace=True)\n",
    "data['model'].replace('PADM00',\"OPPO A3\",inplace=True)\n",
    "data['model'].replace('PBBM00',\"OPPO A7\",inplace=True)\n",
    "data['model'].replace('PAAM00',\"OPPO R15_1\",inplace=True)\n",
    "data['model'].replace('PACT00',\"OPPO R15_2\",inplace=True)\n",
    "data['model'].replace('PABT00',\"OPPO A5_1\",inplace=True)\n",
    "data['model'].replace('PBCM10',\"OPPO R15x\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nginxtime\tbigint\t请求到达服务时间，单位ms\n",
    "# 通过转化成时间戳转化为天和小时的特征\n",
    "# 时间特征\n",
    "data['time'] = pd.to_datetime(data['nginxtime'] /1000,unit='s') + timedelta(hours=8)\n",
    "data['day'] = data['time'].dt.day\n",
    "data['hour'] = data['time'].dt.hour\n",
    "#请求sid 里面有一个 到达的时间搓将时间搓切割出来\n",
    "## 进一步研究时间\n",
    "####sid 时间戳\n",
    "data['sid_timestamp']=data['sid'].apply(lambda x:str(x).split('-')[-1]).astype(str)\n",
    "##请求时间和到达时间相差\n",
    "##判断请求时间和到达时间是否相等\n",
    "data['nginxtime_diff_sid_time']=data['nginxtime'].astype('float')-data['sid_timestamp'].astype('float')\n",
    "data['nginxtime_sid_timestamp'] = (data['nginxtime']==data['sid_timestamp'].astype(str)).astype(int)\n",
    "## 进一步数据预处理\n",
    "##将os字段变成大写\n",
    "data['os'] = data[data.os.notnull()]['os'].apply(lambda x: str(x).upper())\n",
    "data['os'] = data['os'].map(lambda x:str(x).upper())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始开源baseline特征   \n",
    "data['big_model'] = data['model'].map(lambda x: x.split(' ')[0])\n",
    "data['model_equal_make'] = (data['big_model'] == data['make']).astype(int)\n",
    "\n",
    "#制造商和机型进行交叉\n",
    "data['machine'] = data['make'].astype(str) + '-' + data['model'].astype(str)\n",
    "\n",
    "#开源特征https://blog.csdn.net/qq_41776781/article/details/97387261\n",
    "import re\n",
    "data['h'].replace(0.0, np.nan, inplace=True)\n",
    "data['w'].replace(0.0, np.nan, inplace=True)\n",
    "# all_data['ppi'].replace(0.0, np.nan, inplace=True)\n",
    "# cols = ['h', 'w', 'ppi']\n",
    "cols = ['h', 'w']\n",
    "gp_col = 'make'\n",
    "for col in tqdm(cols):\n",
    "    na_series = data[col].isna()\n",
    "    names = list(data.loc[na_series, gp_col])\n",
    "    # 使用均值 或者众数进行填充缺失值\n",
    "    # df_fill = all_data.groupby(gp_col)[col].mean()\n",
    "    df_fill = data.groupby(gp_col)[col].agg(lambda x: stats.mode(x)[0][0])\n",
    "    t = df_fill.loc[names]\n",
    "    t.index = data.loc[na_series, col].index\n",
    "    # 相同的index进行赋值\n",
    "    data.loc[na_series, col] = t\n",
    "    \n",
    "# 开源iphttps://blog.csdn.net/qq_41776781/article/details/97387261\n",
    "data['ip0'] = data['ip'].apply(lambda x:'.'.join(str(x).split('.')[:1]))\n",
    "data['ip1'] = data['ip'].apply(lambda x:'.'.join(str(x).split('.')[0:2]))\n",
    "data['ip2'] = data['ip'].apply(lambda x:'.'.join(str(x).split('.')[0:3]))\n",
    "\n",
    "data['reqrealip0'] = data['reqrealip'].apply(lambda x:'.'.join(str(x).split('.')[:1]))\n",
    "data['reqrealip1'] = data['reqrealip'].apply(lambda x:'.'.join(str(x).split('.')[0:2]))\n",
    "data['reqrealip2'] = data['reqrealip'].apply(lambda x:'.'.join(str(x).split('.')[0:3]))\n",
    "\n",
    "#判断二者是否相等\n",
    "data['ip_reqrealip']   = (data['ip'] == data['reqrealip']).astype(int)\n",
    "data['adidmd5_openudidmd5_d']=(data['adidmd5']==data['openudidmd5']).astype(int)\n",
    "##判断md5是否相等\n",
    "# data['openudidmd5_adidmd5']=(data['openudidmd5']==data['adidmd5']).astype(int) \n",
    "#下面暴力好像掉分\n",
    "# data['ip_equal_1'] = (data['ip1'] == data['reqrealip1']).astype(int)\n",
    "# data['ip_equal_2'] = (data['ip2'] == data['reqrealip2']).astype(int)\n",
    "#继续深挖 osv\n",
    "data['osv0'] = data['osv'].astype(str).map(lambda x:'.'.join(x.split('.')[:1]))\n",
    "data['osv1'] = data['osv'].astype(str).map(lambda x:'.'.join(x.split('.')[0:2]))\n",
    "data['osv2'] = data['osv'].astype(str).map(lambda x:'.'.join(x.split('.')[0:3]))\n",
    "## 判断ip 和reqrealip 是否相等判断欺诈\n",
    "\n",
    "#既然特征在catboost上面表现优越，在原来类别特征之间找新的交叉特征\n",
    "### ip 暴力与设备进行暴力交叉\n",
    "## 找城市，省份欺诈设备族群,ip 和 抬头ip过于稀疏，所以只拿province 和city 和设备\n",
    "###['lan','make','model','osv','dvctype','imeimd5','os']暂时就这些，其他的nunique 过于少\n",
    "#剔除重要性差特征\n",
    "province_city = ['province','city']\n",
    "pc_cos = ['make','model','osv','dvctype','imeimd5','lan','macmd5']#os,\n",
    "for i in province_city:\n",
    "    for j in pc_cos:\n",
    "        data['twoPC_'+str(i)+'_'+str(j)] = data[i].astype(str)+'_'+data[j].astype(str)\n",
    "#媒体暴力交互\n",
    "##媒体信息和设备交互欺诈族群寻找\n",
    "###媒体信息有 ['pkgname','ver','adunitshowid','mediashowid'，'apptype']\n",
    "###设备信息有['adidmd5','imeimd5','idfamd5','openudidmd5','macmd5','model','make','carrier','osv','lan']剔除了仅仅只有os\n",
    "### 将媒体信息与全部设备信息进行暴力交叉\n",
    "Media_Information = ['pkgname','ver','adunitshowid','mediashowid','apptype']#按顺序开始取字段\n",
    "Device_Information = ['adidmd5','imeimd5','openudidmd5','macmd5','model','make','carrier','osv','lan']#,'idfamd5'\n",
    "for ii in Media_Information:\n",
    "    for jj in Device_Information:\n",
    "        data['twoMD_'+str(ii)+'_'+str(jj)] = data[ii].astype(str)+'_'+data[jj].astype(str)\n",
    "#ip 和默认下载匹配app交互暴力交互ip\n",
    "Ip_Information=['ip','reqrealip']\n",
    "Id_Information=['adunitshowid']#'mediashowid','mediashowid',好像过拟合\n",
    "for ii in Ip_Information:\n",
    "    for jj in Id_Information:\n",
    "        data['twoMD_'+str(ii)+'_'+str(jj)] = data[ii].astype(str)+'_'+data[jj].astype(str)\n",
    "## 强特'imeimd5'暴力交叉\n",
    "##设备信息自身类别交互。先选出设备唯一id，一般设备id为International Mobile Equipment Identity IMEI识别码。\n",
    "Za_Information = ['model','osv','ver','dvctype','carrier']#按顺序开始取字段\n",
    "Imd_Information = ['imeimd5']#,'idfamd5',其他ID好像过拟合了\n",
    "for ii in Za_Information:\n",
    "    for jj in Imd_Information:\n",
    "        data['twoMD_'+str(ii)+'_'+str(jj)] = data[ii].astype(str)+'_'+data[jj].astype(str)\n",
    "\n",
    "\n",
    "##设备信息自身类别交互。通过操作系统和本身设备交互\n",
    "### 看看ime5和其他设备和部分设备是如何交互\n",
    "Device_Information3=['model','make','ntt']\n",
    "for iiii in Device_Information3:\n",
    "    data['twoMD_'+str('osv')+'_'+str(iiii)] = data['osv'].astype(str)+'_'+data[iiii].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['size'] = (np.sqrt(data['h'] ** 2 + data['w'] ** 2) / 2.54) / 1000\n",
    "data['ratio'] = data['h'] / data['w']\n",
    "data['px'] = data['ppi'] * data['size']\n",
    "data['mj'] = data['h'] * data['w']\n",
    "\n",
    "###查看该用户id用过多少个机型和设备等等(人与人的交互)开源特征：\n",
    "sid_cro = ['ver','apptype','ntt','h','w','ppi','ratio','city','dvctype']\n",
    "for x in sid_cro:\n",
    "    data['sid'+'_'+'use'+'_'+str(x)+'_count'] = data.groupby([x])['sid'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = [i for i in data.columns if i not in ['sid', 'label', 'time', 'day']]\n",
    "cat_list = [i for i in cat_col if i not in ['nginxtime']]\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = feature_name\n",
    "all_zero_feature=[]\n",
    "X_train = data[data['label']!=-999]\n",
    "y = data[data['label']!=-999]['label']\n",
    "X_test = data[data['label']==-999]\n",
    "print(X_train.shape,X_test.shape)\n",
    "oof = np.zeros(X_train.shape[0])\n",
    "prediction = np.zeros(X_test.shape[0])\n",
    "seeds = [2018, 2019,4096, 2048, 1024]\n",
    "num_model_seed = 1\n",
    "for model_seed in range(num_model_seed):\n",
    "    oof_cat = np.zeros(X_train.shape[0])\n",
    "    prediction_cat=np.zeros(X_test.shape[0])\n",
    "    skf = StratifiedKFold(n_splits=7, random_state=seeds[model_seed], shuffle=True)\n",
    "    for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "        print(index)\n",
    "        train_x, test_x, train_y, test_y = X_train[feature_name].iloc[train_index], X_train[feature_name].iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "        print(train_x.shape)\n",
    "        print(train_y.shape)\n",
    "        print(test_x.shape)\n",
    "        print(test_y.shape)\n",
    "        cbt_model = cbt.CatBoostClassifier(iterations=5000,learning_rate=0.1,max_depth=7,\n",
    "                                           l2_leaf_reg=1,verbose=100,early_stopping_rounds=500,task_type='GPU',eval_metric='F1',cat_features=cat_list)\n",
    "        cbt_model.fit(train_x[feature_name], train_y,eval_set=(test_x[feature_name],test_y))            \n",
    "        oof_cat[test_index] += cbt_model.predict_proba(test_x)[:,1]\n",
    "        prediction_cat += cbt_model.predict_proba(X_test[feature_name])[:,1]/7       \n",
    "        featureimportance=cbt_model.get_feature_importance(prettified=True)   \n",
    "        importance_0=list(featureimportance[featureimportance['Importances']==0]['Feature Index'])\n",
    "        all_zero_feature.extend(importance_0)\n",
    "        feature_importances['fold-{}'.format(index+1)] = cbt_model.get_feature_importance()\n",
    "        featureimportance.to_csv(path+'feats{}.csv'.format(index),index=False)        \n",
    "    print('F1',f1_score(y, np.round(oof_cat)))    \n",
    "    oof += oof_cat / num_model_seed\n",
    "    prediction += prediction_cat / num_model_seed\n",
    "print('score',f1_score(y, np.round(oof))) \n",
    "# write to csv\n",
    "submit = test[['sid']]\n",
    "submit['label'] =prediction\n",
    "submit.to_csv(path+\"tmp/bs_gl1.csv\",index=False)\n",
    "submit['label'] = (prediction>=0.5).astype(int)\n",
    "print(submit['label'].value_counts())\n",
    "submit.to_csv(path+\"tmp/dt1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
